{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to download and import tensor. The library used to import ELMO to create the necessary embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"tensorflow_hub>=0.6.0\"\n",
    "# !pip install \"tensorflow>=2.0.0\"\n",
    "\n",
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the elmo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elmo = hub.KerasLayer(\"https://tfhub.dev/google/elmo/2\", trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function used to create embeddings in batches in order to avoid running out of ram\n",
    "Reference: AnalyticsVidya https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def elmo_vectors(x):\n",
    "#   embeddings = elmo(x.tolist())\n",
    "#   with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     sess.run(tf.tables_initializer())\n",
    "#     # return average of ELMo features\n",
    "#     return sess.run(tf.reduce_mean(embeddings,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing dataset and using only 75,000 vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['talesfromtechsupport' 'teenmom' 'Harley' 'ringdoorbell' 'intel'\n",
      " 'residentevil' 'BATProject' 'hockeyplayers' 'asmr' 'rawdenim'\n",
      " 'steinsgate' 'DBZDokkanBattle' 'Nootropics' 'l5r' 'NameThatSong'\n",
      " 'homeless' 'antidepressants' 'absolver' 'KissAnime' 'sissyhypno'\n",
      " 'oculusnsfw' 'dpdr' 'Garmin' 'AskLiteraryStudies' 'poetry_critics'\n",
      " 'skiing' 'shrimptank' 'logorequests' 'Stargate' 'foreskin_restoration'\n",
      " 'sharepoint' 'synthesizers' 'gravityfalls' 'androiddev' 'Grimdawn'\n",
      " 'driving' 'FORTnITE' 'dndnext' 'Magic' 'MtvChallenge' 'FoWtcg'\n",
      " 'harrypotter' 'TryingForABaby' 'sewing' 'foxholegame' 'madmen'\n",
      " 'JUSTNOMIL' 'APStudents' 'sharditkeepit' 'amateurradio' 'sleeptrain'\n",
      " 'fatpeoplestories' 'GameStop' 'scuba' 'Firefighting' 'Mustang'\n",
      " 'riverdale' 'flying' 'bartenders' 'scooters' 'trumpet' 'projecteternity'\n",
      " 'musictheory' 'factorio' 'SexToys' 'EternalCardGame' 'PLC' 'sailing'\n",
      " 'Mattress' 'climbing' 'uberdrivers' 'Cloud9' 'csharp' 'communism101'\n",
      " 'windowsphone']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"rspct.tsv\", sep='\\t')\n",
    "print(df.subreddit.unique()[0:75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75000\n"
     ]
    }
   ],
   "source": [
    "df.head\n",
    "df = df.loc[df['subreddit'].isin(df.subreddit.unique()[0:75])]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions used to create embeddings of reddit title and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm_notebook\n",
    "# reddit_posts = [df[i:i+1000] for i in range(0,df.shape[0],1000)]\n",
    "# title_embeddings =[]\n",
    "# import pickle\n",
    "\n",
    "# for i in tqdm_notebook(reddit_posts):\n",
    "# #     print(i['title'].tolist())\n",
    "#     tensor = tf.convert_to_tensor(\n",
    "#         i['title'].tolist(),\n",
    "#         dtype=None,\n",
    "#         dtype_hint=None,\n",
    "#         name=None\n",
    "#     )\n",
    "#     title_embeddings.append(elmo(tensor))\n",
    "#     with open('redd_tittle.pickle', 'wb') as f:\n",
    "#         pickle.dump(title_embeddings, f)\n",
    "# print(title_embeddings)\n",
    "# # print() \n",
    "# # elmo_title_vecs = [elmo_vectors(x['title']) for x in tqdm(df.iloc[0:10])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post_text_embeddings =[]\n",
    "# import pickle\n",
    "# from tqdm import tqdm_notebook\n",
    "\n",
    "# reddit_posts = [df[i:i+100] for i in range(0,df.shape[0],100)]\n",
    "# post_text_embeddings = []\n",
    "# for i in tqdm_notebook(reddit_posts):\n",
    "#     tensor = tf.convert_to_tensor(\n",
    "#         i['selftext'].tolist(),\n",
    "#         dtype=None,\n",
    "#         dtype_hint=None,\n",
    "#         name=None\n",
    "#     )\n",
    "#     post_text_embeddings.append(elmo(tensor))\n",
    "#     with open('redd_self_text.pickle', 'wb') as f:\n",
    "#         pickle.dump(post_text_embeddings, f)\n",
    "# print(post_text_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling vectors to machine and concating all subarrays into a large array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n",
      "75000\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "infile = open('redd_tittle.pickle','rb')\n",
    "arr = pickle.load(infile)\n",
    "print(len(arr))\n",
    "infile.close()\n",
    "\n",
    "import numpy as np\n",
    "elmo_title_vecs_new = np.concatenate(arr, axis = 0)\n",
    "print(len(elmo_title_vecs_new))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting dataset into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_title, test_title, train_subreddit, test_subreddit = train_test_split(elmo_title_vecs_new,\n",
    "                                                                      df[\"subreddit\"],\n",
    "                                                                      test_size=0.1, \n",
    "                                                                      random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/ubuntu/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# train_title.head\n",
    "# train_subreddit.head\n",
    "lreg = LogisticRegression()\n",
    "lreg.fit(train_title, train_subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4945333333333333\n"
     ]
    }
   ],
   "source": [
    "preds_valid = lreg.predict(test_title)\n",
    "correct = 0\n",
    "for actual, pred in zip(test_subreddit, preds_valid):\n",
    "  if actual == pred:\n",
    "    correct = correct + 1\n",
    "print(correct/len(test_subreddit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickling Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pickle', 'wb') as f:\n",
    "    pickle.dump(lreg, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('predictions.pickle', 'wb') as f:\n",
    "    pickle.dump(preds_valid, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Measuring K Nearest Accuracy of Model on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AskLiteraryStudies': 121, 'l5r': 91, 'Garmin': 112, 'sewing': 103, 'fatpeoplestories': 97, 'MtvChallenge': 109, 'Harley': 88, 'PLC': 96, 'Mustang': 86, 'bartenders': 98, 'amateurradio': 93, 'Cloud9': 109, 'csharp': 87, 'oculusnsfw': 116, 'gravityfalls': 88, 'scuba': 86, 'FoWtcg': 108, 'BATProject': 93, 'hockeyplayers': 99, 'androiddev': 124, 'trumpet': 91, 'EternalCardGame': 77, 'JUSTNOMIL': 112, 'Stargate': 81, 'Nootropics': 107, 'Mattress': 115, 'teenmom': 106, 'sleeptrain': 95, 'foxholegame': 87, 'musictheory': 113, 'poetry_critics': 103, 'TryingForABaby': 98, 'DBZDokkanBattle': 102, 'talesfromtechsupport': 107, 'Firefighting': 83, 'projecteternity': 86, 'riverdale': 99, 'madmen': 100, 'synthesizers': 92, 'climbing': 95, 'uberdrivers': 97, 'Magic': 109, 'APStudents': 107, 'sailing': 100, 'driving': 112, 'shrimptank': 109, 'windowsphone': 89, 'steinsgate': 123, 'rawdenim': 103, 'homeless': 116, 'foreskin_restoration': 94, 'sharditkeepit': 98, 'ringdoorbell': 95, 'residentevil': 95, 'antidepressants': 103, 'Grimdawn': 109, 'flying': 89, 'communism101': 84, 'KissAnime': 95, 'asmr': 107, 'absolver': 91, 'logorequests': 95, 'factorio': 107, 'dpdr': 125, 'sissyhypno': 92, 'scooters': 100, 'dndnext': 100, 'harrypotter': 96, 'sharepoint': 129, 'GameStop': 103, 'SexToys': 97, 'FORTnITE': 65, 'skiing': 88, 'intel': 103, 'NameThatSong': 122}\n",
      "K-Precision at k =1\n",
      "0.5785123966942148\n",
      "K-Precision at k =3\n",
      "0.5061728395061729\n",
      "K-Precision at k =5\n",
      "0.8523076923076923\n"
     ]
    }
   ],
   "source": [
    "counter_dict = {}\n",
    "for pred in preds_valid:\n",
    "    if pred in counter_dict:\n",
    "        counter_dict[pred] = counter_dict[pred] +1\n",
    "    else:\n",
    "        counter_dict[pred] = 1\n",
    "print(counter_dict)\n",
    "\n",
    "print(\"K-Precision at k =1\")\n",
    "k1Counter = 0\n",
    "k1CorrectCounter = 0\n",
    "for actual, pred in zip(test_subreddit, preds_valid):\n",
    "    if pred == 'AskLiteraryStudies':\n",
    "        k1Counter = k1Counter + 1\n",
    "        if pred == actual:\n",
    "            k1CorrectCounter = k1CorrectCounter + 1\n",
    "print(k1CorrectCounter/k1Counter)\n",
    "\n",
    "\n",
    "print(\"K-Precision at k =3\")\n",
    "k3Counter = 0\n",
    "k3CorrectCounter = 0\n",
    "for actual, pred in zip(test_subreddit, preds_valid):\n",
    "    if pred == 'AskLiteraryStudies' or pred == 'l5r' or pred == 'Garmin':\n",
    "        k3Counter = k3Counter + 1\n",
    "        if pred == actual:\n",
    "            k3CorrectCounter = k3CorrectCounter + 1\n",
    "print(k3CorrectCounter/k3Counter)\n",
    "\n",
    "print(\"K-Precision at k =5\")\n",
    "k5Counter = 0\n",
    "k5CorrectCounter = 0\n",
    "for actual, pred in zip(test_subreddit, preds_valid):\n",
    "    if pred in ['AskLiteraryStudies', 'l5r', 'Garmin', 'sewing', 'fatpeoplestories']:\n",
    "        k5Counter = k3Counter + 1\n",
    "        if pred == actual:\n",
    "            k5CorrectCounter = k5CorrectCounter + 1\n",
    "print(k5CorrectCounter/k5Counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
